{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports Complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "from collections import namedtuple\n",
    "import time\n",
    "import os\n",
    "from dzlib.utils.helper import info, stats, npshow, params, janimate, ccrop_pil\n",
    "from dzlib.nn_models.unet import UNet\n",
    "\n",
    "print(f\"Imports Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Settings\n",
    "dtype = torch.FloatTensor\n",
    "%matplotlib notebook\n",
    "matplotlib.rcParams['savefig.dpi'] = 80\n",
    "matplotlib.rcParams['figure.dpi'] = 80\n",
    "# %config InlineBackend.print_figure_kwargs={'bbox_inches':None}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960, 609)\n",
      "(960, 608)\n",
      "(240, 152)\n",
      "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n"
     ]
    }
   ],
   "source": [
    "# image_pil (original)\n",
    "data_dir = os.getcwd() + \"/data\"\n",
    "fn = data_dir + \"/dog.jpg\"\n",
    "image_pil = Image.open(fn)\n",
    "width, height = image_pil.size\n",
    "print(image_pil.size)\n",
    "\n",
    "# Center Crop dims to be divisible by 32\n",
    "image_pil_HR = ccrop_pil(image_pil, factor=32)\n",
    "print(image_pil_HR.size)\n",
    "\n",
    "\n",
    "\n",
    "# image_pil_LR (Resize by factor of 4)\n",
    "factor = 4\n",
    "LR_shape = (int(width / factor), int(height / factor))\n",
    "image_pil_LR = image_pil_HR.resize(LR_shape, Image.ANTIALIAS)\n",
    "width, height = image_pil_LR.size\n",
    "print(image_pil_LR.size)\n",
    "    \n",
    "print(type(image_pil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_np_HR (np > float32 > (C x H x W) > (0...1))\n",
    "image_np_HR = np.asarray(image_pil_HR).astype(np.int32).transpose(2, 0, 1) / 255\n",
    "info(image_np_HR, 'image_np_HR')\n",
    "stats(image_np_HR, 'image_np_HR')\n",
    "\n",
    "image_pt_HR = torch.from_numpy(image_np_HR).type(dtype)\n",
    "info(image_pt_HR, 'image_pt_HR')\n",
    "stats(image_pt_HR, 'image_pt_HR')\n",
    "\n",
    "# image_np_LR (np > float32 > (C x H x W) > (0...1))\n",
    "image_np_LR = np.asarray(image_pil_LR).astype(np.int32).transpose(2, 0, 1) / 255\n",
    "info(image_np_LR, 'image_np_LR')\n",
    "stats(image_np_LR, 'image_np_LR')\n",
    "\n",
    "image_pt_LR = torch.from_numpy(image_np_LR).type(dtype)\n",
    "info(image_pt_LR, 'image_pt_LR')\n",
    "stats(image_pt_LR, 'image_pt_LR')\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(14, 8), ncols=2)\n",
    "npshow(image_np_HR, ax[0])\n",
    "npshow(image_np_LR, ax[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ninput = torch.zeros(1, 32, image_pt_HR.shape[1], image_pt_HR.shape[2])\n",
    "std_scaler = 1./10.\n",
    "reg_noise_std = 0.03\n",
    "ninput = ninput.uniform_() * std_scaler\n",
    "ninput = ninput.type(dtype)\n",
    "ninput_saved = ninput.detach().clone()\n",
    "noise = ninput.detach().clone()\n",
    "\n",
    "target = image_pt_LR.view(1, *image_pt_LR.shape).type(dtype)\n",
    "\n",
    "info(net_input)\n",
    "stats(net_input)\n",
    "\n",
    "info(target)\n",
    "stats(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(UNet.__init__.__doc__)\n",
    "print(UNet.activations)\n",
    "print(UNet.Conv._fields)\n",
    "print(UNet.Upsample._fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = 'reflect'\n",
    "\n",
    "in_channels = ninput.shape[1]\n",
    "out_channels = target.shape[1]\n",
    "\n",
    "down_channels = [128, 128, 128, 128, 128]\n",
    "skip_channels = [4, 4, 4, 4, 4]\n",
    "up_channels = [128, 128, 128, 128, 128]\n",
    "\n",
    "down_conv1 = UNet.Conv(3, 2, pad)\n",
    "down_conv2 = UNet.Conv(3, 1, pad)\n",
    "down_convs = [down_conv1, down_conv2]\n",
    "\n",
    "skip_conv1 = UNet.Conv(1, 1, pad)\n",
    "skip_convs = [skip_conv1]\n",
    "\n",
    "up_conv1 = UNet.Conv(3, 1, pad)\n",
    "up_conv2 = UNet.Conv(1, 1, pad)\n",
    "up_convs = [up_conv1, up_conv2]\n",
    "\n",
    "batchnorm = True\n",
    "last_batchnorm = False\n",
    "\n",
    "activation = 'leakyrelu'\n",
    "last_activation = 'sigmoid'\n",
    "\n",
    "upsample = UNet.Upsample(size=None, scale_factor=2, mode='bilinear', align_corners=None)\n",
    "\n",
    "net = UNet(in_channels, out_channels, down_channels, skip_channels, up_channels, \\\n",
    "           down_convs, skip_convs, up_convs, batchnorm, last_batchnorm, activation, last_activation, upsample)\n",
    "net = net.type(dtype)\n",
    "\n",
    "n_params = params(net)\n",
    "print(n_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 2000\n",
    "cwd = os.getcwd()\n",
    "save_path = cwd+'/outputs' + '/run3'\n",
    "os.mkdir(save_path)\n",
    "\n",
    "digits = len(str(n_iters))\n",
    "filenames = [f\"{save_path}/{i+1:0{digits}d}.png\" for i in range(n_iters)]\n",
    "\n",
    "print(len(filenames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 16))\n",
    "gs = GridSpec(nrows=6, ncols=1, figure=fig)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[:2, :])\n",
    "ax1.set_xlim(0, image_np_HR.shape[2]*2)\n",
    "ax1.set_ylim(image_np_HR.shape[1], 0)\n",
    "kwargs1 = None\n",
    "\n",
    "ax2 = fig.add_subplot(gs[2:4, :])\n",
    "ax2.set_xlim(0, image_np_LR.shape[2]*2)\n",
    "ax2.set_ylim(image_np_LR.shape[1], 0)\n",
    "kwargs1 = None\n",
    "\n",
    "ax3 = fig.add_subplot(gs[4:, :])\n",
    "kwargs3 = {'color': 'k'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "mse = nn.MSELoss().type(dtype)\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "losses = []\n",
    "remove = True\n",
    "\n",
    "train_time = time.time()\n",
    "for i in range(n_iters):\n",
    "    iter_time = time.time()\n",
    "    \n",
    "    # zero grad\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # reg\n",
    "    ninput = ninput_saved + noise.normal_() * reg_noise_std\n",
    "    \n",
    "    # forward pass\n",
    "    out_HR = net(ninput)\n",
    "    \n",
    "    # downsample\n",
    "    out_LR = F.interpolate(input=out_HR, size=(target.shape[2], target.shape[3]), mode='bilinear')\n",
    "    \n",
    "    # evaluate loss\n",
    "    loss = mse(out_LR, target)\n",
    "    \n",
    "    # back prop\n",
    "    loss.backward()\n",
    "    \n",
    "    # step\n",
    "    optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dz_standard] *",
   "language": "python",
   "name": "conda-env-dz_standard-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
